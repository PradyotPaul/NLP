{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12519385,"sourceType":"datasetVersion","datasetId":7902512}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nimport sentencepiece as spm\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn.utils.rnn import pad_sequence\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom collections import Counter  # ADD THIS IMPORT\n\n# Set device and ensure reproducibility\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed(42)\n\n# --- Data Loading and Preprocessing ---\n# Update the file path - make it more flexible\ntry:\n    df = pd.read_csv('/kaggle/input/asm-eng/Filtered_data.tsv', sep='\\t', names=['asm', 'eng'], on_bad_lines='skip')\nexcept FileNotFoundError:\n    print(\"Dataset not found at /content/Filtered_data.tsv\")\n    print(\"Please ensure the dataset file exists or update the path\")\n    # Create dummy data for testing if file not found\n    df = pd.DataFrame({\n        'asm': ['মই ঘৰত থাকোঁ।', 'তেওঁ আজি বিদ্যালয়লৈ গ\\'ল।'],\n        'eng': ['I am at home.', 'He went to school today.']\n    })\n\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n\nwith open('all_text_for_bpe.txt', 'w', encoding='utf-8') as f:\n    for text in pd.concat([train_df['asm'], train_df['eng']]):\n        f.write(str(text).strip().lower() + '\\n')\n\nspm.SentencePieceTrainer.Train(\n    '--input=all_text_for_bpe.txt --model_prefix=spm_bpe --vocab_size=8000 '\n    '--character_coverage=1.0 --model_type=bpe'\n)\nsp = spm.SentencePieceProcessor()\nsp.Load('spm_bpe.model')\n\nPAD_IDX = 0\nSOS_IDX = sp.bos_id()\nEOS_IDX = sp.eos_id()\nVOCAB_SIZE = sp.GetPieceSize()\n\n# --- Display Vocabulary Information ---\nprint(f\"\\n=== VOCABULARY INFORMATION ===\")\nprint(f\"Vocabulary size: {VOCAB_SIZE}\")\nprint(f\"PAD token: {PAD_IDX} -> '{sp.id_to_piece(PAD_IDX)}'\")\nprint(f\"SOS token: {SOS_IDX} -> '{sp.id_to_piece(SOS_IDX)}'\")\nprint(f\"EOS token: {EOS_IDX} -> '{sp.id_to_piece(EOS_IDX)}'\")\nprint(f\"UNK token: {sp.unk_id()} -> '{sp.id_to_piece(sp.unk_id())}'\")\n\n# Display sample vocabulary\nprint(f\"\\nSample vocabulary (first 50 tokens):\")\nfor i in range(min(50, VOCAB_SIZE)):\n    piece = sp.id_to_piece(i)\n    print(f\"{i:3d}: '{piece}'\")\n\n# Display high-frequency tokens\nprint(f\"\\nHigh-frequency tokens (500-600):\")\nfor i in range(500, min(600, VOCAB_SIZE)):\n    piece = sp.id_to_piece(i)\n    print(f\"{i:3d}: '{piece}'\")\n\n# Test tokenization with sample sentences\nprint(f\"\\n=== TOKENIZATION EXAMPLES ===\")\ntest_sentences = [\n    \"মই ঘৰত থাকোঁ।\",\n    \"তেওঁ আজি বিদ্যালয়লৈ গ'ল।\",\n    \"বইখন টেবুলৰ ওপৰত আছে।\",\n    \"I am at home.\",\n    \"He went to school today.\",\n    \"The book is on the table.\"\n]\n\nfor sentence in test_sentences:\n    tokens = sp.encode_as_pieces(sentence.lower().strip())\n    token_ids = sp.encode_as_ids(sentence.lower().strip())\n    print(f\"\\nSentence: {sentence}\")\n    print(f\"Tokens: {tokens}\")\n    print(f\"Token IDs: {token_ids}\")\n    print(f\"Reconstructed: {sp.decode_pieces(tokens)}\")\n\n# --- Dataset and DataLoader ---\nclass TranslationDataset(Dataset):\n    def __init__(self, df, sp_model, max_len=100):\n        self.src_sents = df['asm'].astype(str).tolist()\n        self.trg_sents = df['eng'].astype(str).tolist()\n        self.sp_model = sp_model\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.src_sents)\n\n    def __getitem__(self, idx):\n        src_encoded = self.sp_model.EncodeAsIds(self.src_sents[idx].lower().strip())\n        trg_encoded = self.sp_model.EncodeAsIds(self.trg_sents[idx].lower().strip())\n        src_tensor = torch.LongTensor([SOS_IDX] + src_encoded[:self.max_len-2] + [EOS_IDX])\n        trg_tensor = torch.LongTensor([SOS_IDX] + trg_encoded[:self.max_len-2] + [EOS_IDX])\n        return src_tensor, trg_tensor\n\ndef collate_fn(batch):\n    srcs, trgs = zip(*batch)\n    src_padded = pad_sequence(srcs, batch_first=True, padding_value=PAD_IDX)\n    trg_padded = pad_sequence(trgs, batch_first=True, padding_value=PAD_IDX)\n    return src_padded, trg_padded\n\nBATCH_SIZE = 32  # Increased for stability\ntrain_loader = DataLoader(TranslationDataset(train_df, sp), batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\nval_loader = DataLoader(TranslationDataset(val_df, sp), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\ntest_loader = DataLoader(TranslationDataset(test_df, sp), batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n\n# Move analysis functions here and call them\ndef analyze_vocabulary(sp_model, train_df, val_df):\n    \"\"\"Analyze vocabulary coverage and statistics\"\"\"\n    print(f\"\\n=== VOCABULARY ANALYSIS ===\")\n    \n    # Collect all unique tokens\n    all_tokens = set()\n    asm_tokens = set()\n    eng_tokens = set()\n    \n    # Analyze training data\n    for _, row in train_df.iterrows():\n        asm_toks = sp_model.encode_as_pieces(str(row['asm']).lower().strip())\n        eng_toks = sp_model.encode_as_pieces(str(row['eng']).lower().strip())\n        \n        all_tokens.update(asm_toks)\n        all_tokens.update(eng_toks)\n        asm_tokens.update(asm_toks)\n        eng_tokens.update(eng_toks)\n    \n    print(f\"Total unique tokens in training: {len(all_tokens)}\")\n    print(f\"Assamese tokens: {len(asm_tokens)}\")\n    print(f\"English tokens: {len(eng_tokens)}\")\n    print(f\"Vocabulary coverage: {len(all_tokens)/sp_model.GetPieceSize()*100:.1f}%\")\n    \n    # Find most common tokens\n    token_counts = Counter()\n    \n    for _, row in train_df.iterrows():\n        tokens = sp_model.encode_as_pieces(str(row['asm']).lower().strip())\n        tokens.extend(sp_model.encode_as_pieces(str(row['eng']).lower().strip()))\n        token_counts.update(tokens)\n    \n    print(f\"\\nTop 20 most frequent tokens:\")\n    for token, count in token_counts.most_common(20):\n        try:\n            token_id = sp_model.piece_to_id(token)\n            print(f\"'{token}' (ID: {token_id}): {count} times\")\n        except:\n            print(f\"'{token}': {count} times (ID not found)\")\n\ndef analyze_language_distribution(sp_model, train_df):\n    \"\"\"Analyze how vocabulary is distributed between languages\"\"\"\n    print(f\"\\n=== LANGUAGE DISTRIBUTION IN VOCABULARY ===\")\n    \n    # Count token frequency by language\n    asm_token_counts = Counter()\n    eng_token_counts = Counter()\n    \n    # Analyze Assamese sentences\n    for text in train_df['asm'].astype(str):\n        tokens = sp_model.encode_as_pieces(text.lower().strip())\n        asm_token_counts.update(tokens)\n    \n    # Analyze English sentences  \n    for text in train_df['eng'].astype(str):\n        tokens = sp_model.encode_as_pieces(text.lower().strip())\n        eng_token_counts.update(tokens)\n    \n    # Find shared vs unique tokens\n    asm_tokens = set(asm_token_counts.keys())\n    eng_tokens = set(eng_token_counts.keys())\n    shared_tokens = asm_tokens & eng_tokens\n    asm_only = asm_tokens - eng_tokens\n    eng_only = eng_tokens - asm_tokens\n    \n    print(f\"Total vocabulary size: {sp_model.GetPieceSize()}\")\n    print(f\"Tokens used in Assamese: {len(asm_tokens)}\")\n    print(f\"Tokens used in English: {len(eng_tokens)}\")\n    print(f\"Shared tokens: {len(shared_tokens)}\")\n    print(f\"Assamese-only tokens: {len(asm_only)}\")\n    print(f\"English-only tokens: {len(eng_only)}\")\n    \n    print(f\"\\nTop 10 shared tokens:\")\n    shared_counts = {token: asm_token_counts[token] + eng_token_counts[token] \n                    for token in shared_tokens}\n    for token, count in sorted(shared_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"  '{token}': {count} times (Asm: {asm_token_counts[token]}, Eng: {eng_token_counts[token]})\")\n    \n    print(f\"\\nTop 10 Assamese-only tokens:\")\n    asm_only_sorted = [(token, asm_token_counts[token]) for token in asm_only]\n    asm_only_sorted.sort(key=lambda x: x[1], reverse=True)\n    for token, count in asm_only_sorted[:10]:\n        print(f\"  '{token}': {count} times\")\n    \n    print(f\"\\nTop 10 English-only tokens:\")\n    eng_only_sorted = [(token, eng_token_counts[token]) for token in eng_only]\n    eng_only_sorted.sort(key=lambda x: x[1], reverse=True)\n    for token, count in eng_only_sorted[:10]:\n        print(f\"  '{token}': {count} times\")\n\n# Call analysis functions immediately after defining them\nanalyze_vocabulary(sp, train_df, val_df)\nanalyze_language_distribution(sp, train_df)\n\n# --- Model Definition ---\n# --- BiLSTM Encoder ---\nclass BiLSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IDX)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Single bidirectional LSTM layer for simplicity\n        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=1, bidirectional=True, \n                           dropout=0, batch_first=True)\n        \n        # Layer normalization for stability\n        self.layer_norm = nn.LayerNorm(hid_dim * 2)\n        \n        # Project to decoder size\n        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)\n        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)\n\n    def forward(self, src):\n        # Get actual sequence lengths for packing\n        src_lengths = (src != PAD_IDX).sum(dim=1).cpu()\n        \n        embedded = self.dropout(self.embedding(src))\n        \n        # Pack sequences for efficiency\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded, src_lengths, batch_first=True, enforce_sorted=False)\n        \n        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n        \n        # Unpack sequences\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        \n        # Apply layer normalization\n        outputs = self.layer_norm(outputs)\n        \n        # Process final states - hidden/cell are [2, batch, hid_dim] for bidirectional\n        # Concatenate forward and backward final states\n        final_hidden = torch.cat((hidden[0], hidden[1]), dim=1)  # [batch, hid_dim*2]\n        final_cell = torch.cat((cell[0], cell[1]), dim=1)        # [batch, hid_dim*2]\n        \n        # Project to decoder dimensions\n        decoder_hidden = torch.tanh(self.fc_hidden(final_hidden)).unsqueeze(0)  # [1, batch, hid_dim]\n        decoder_cell = torch.tanh(self.fc_cell(final_cell)).unsqueeze(0)        # [1, batch, hid_dim]\n        \n        return outputs, (decoder_hidden, decoder_cell)\n\nclass LSTMAttention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        self.hid_dim = hid_dim\n        \n        # Attention mechanism\n        self.W_h = nn.Linear(hid_dim, hid_dim, bias=False)  # For hidden state\n        self.W_s = nn.Linear(hid_dim * 2, hid_dim, bias=False)  # For encoder outputs\n        self.v = nn.Linear(hid_dim, 1, bias=False)\n        \n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, hidden_cell, encoder_outputs):\n        # Extract hidden state\n        if isinstance(hidden_cell, tuple):\n            hidden = hidden_cell[0]  # [1, batch, hid_dim]\n        else:\n            hidden = hidden_cell\n        \n        # Remove layer dimension if present\n        if hidden.dim() == 3:\n            hidden = hidden.squeeze(0)  # [batch, hid_dim]\n        \n        batch_size, src_len, enc_hid_dim = encoder_outputs.size()\n        \n        # Prepare hidden state for attention computation\n        hidden_transformed = self.W_h(hidden)  # [batch, hid_dim]\n        hidden_expanded = hidden_transformed.unsqueeze(1).expand(-1, src_len, -1)  # [batch, src_len, hid_dim]\n        \n        # Transform encoder outputs\n        encoder_transformed = self.W_s(encoder_outputs)  # [batch, src_len, hid_dim]\n        \n        # Compute attention scores\n        energy = torch.tanh(hidden_expanded + encoder_transformed)  # [batch, src_len, hid_dim]\n        attention_scores = self.v(energy).squeeze(2)  # [batch, src_len]\n        \n        # Create mask for padding tokens\n        mask = (encoder_outputs.sum(dim=2) != 0)  # [batch, src_len]\n        attention_scores = attention_scores.masked_fill(~mask, -1e10)\n        \n        # Apply softmax\n        attention_weights = torch.softmax(attention_scores, dim=1)  # [batch, src_len]\n        attention_weights = self.dropout(attention_weights)\n        \n        return attention_weights\n\nclass LSTMDecoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, dropout, attention):\n        super().__init__()\n        self.attention = attention\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IDX)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Single layer LSTM\n        self.lstm = nn.LSTM(emb_dim + hid_dim * 2, hid_dim, num_layers=1, batch_first=True)\n        \n        # Output projection with residual connection\n        self.fc_out = nn.Linear(hid_dim + hid_dim * 2 + emb_dim, vocab_size)\n        \n        # Layer normalization\n        self.layer_norm = nn.LayerNorm(hid_dim)\n\n    def forward(self, input, hidden_cell, encoder_outputs):\n        input = input.unsqueeze(1)  # [batch, 1]\n        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n        \n        # Compute attention\n        attention_weights = self.attention(hidden_cell, encoder_outputs)  # [batch, src_len]\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch, 1, hid_dim*2]\n        \n        # Combine embedding and context\n        rnn_input = torch.cat((embedded, context), dim=2)  # [batch, 1, emb_dim + hid_dim*2]\n        \n        # LSTM forward pass\n        output, new_hidden_cell = self.lstm(rnn_input, hidden_cell)  # [batch, 1, hid_dim]\n        \n        # Apply layer normalization\n        output = self.layer_norm(output)\n        \n        # Prepare for output projection\n        embedded_flat = embedded.squeeze(1)  # [batch, emb_dim]\n        output_flat = output.squeeze(1)      # [batch, hid_dim]\n        context_flat = context.squeeze(1)    # [batch, hid_dim*2]\n        \n        # Final output projection\n        prediction = self.fc_out(torch.cat((output_flat, context_flat, embedded_flat), dim=1))\n        \n        return prediction, new_hidden_cell\n\nclass BiLSTMSeq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.fc_out.out_features\n        \n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden_cell = self.encoder(src)\n        \n        input_token = trg[:, 0]\n        \n        for t in range(1, trg_len):\n            output, hidden_cell = self.decoder(input_token, hidden_cell, encoder_outputs)\n            outputs[:, t] = output\n            \n            # Teacher forcing with scheduled sampling\n            if random.random() < teacher_forcing_ratio:\n                input_token = trg[:, t]\n            else:\n                input_token = output.argmax(1)\n            \n        return outputs\n\n# --- Training and Evaluation Functions ---\ndef train_epoch(model, dataloader, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    total_norm = 0\n    \n    for src, trg in dataloader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model(src, trg)\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        loss = criterion(output, trg)\n        loss.backward()\n        \n        # Monitor gradient norms\n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        total_norm += grad_norm.item()\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    avg_grad_norm = total_norm / len(dataloader)\n    if avg_grad_norm > 5.0:  # Warning threshold\n        print(f\"Warning: High gradient norm: {avg_grad_norm:.2f}\")\n    \n    return epoch_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    epoch_loss = 0\n    all_refs, all_hyps = [], []\n    smooth_fn = SmoothingFunction().method1\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(device), trg.to(device)\n            output = model(src, trg, 0)\n            output_dim = output.shape[-1]\n            loss_output = output[:, 1:].reshape(-1, output_dim)\n            loss_trg = trg[:, 1:].reshape(-1)\n            loss = criterion(loss_output, loss_trg)\n            epoch_loss += loss.item()\n            hyp_tokens = output.argmax(2)\n            for i in range(hyp_tokens.shape[0]):\n                hyp_ids = hyp_tokens[i, 1:].tolist()\n                ref_ids = trg[i, 1:].tolist()\n                if EOS_IDX in hyp_ids:\n                    hyp_ids = hyp_ids[:hyp_ids.index(EOS_IDX)]\n                if EOS_IDX in ref_ids:\n                    ref_ids = ref_ids[:ref_ids.index(EOS_IDX)]\n                all_hyps.append(sp.decode_ids(hyp_ids).split())\n                all_refs.append([sp.decode_ids(ref_ids).split()])\n    bleu = corpus_bleu(all_refs, all_hyps, smoothing_function=smooth_fn)\n    return epoch_loss / len(dataloader), bleu * 100\n\ndef train_epoch_with_tf(model, dataloader, optimizer, criterion, clip, tf_ratio):\n    \"\"\"Training function with dynamic teacher forcing ratio\"\"\"\n    model.train()\n    epoch_loss = 0\n    total_norm = 0\n    \n    for src, trg in dataloader:\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model(src, trg, teacher_forcing_ratio=tf_ratio)  # Use dynamic TF ratio\n        output_dim = output.shape[-1]\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n        loss = criterion(output, trg)\n        loss.backward()\n        \n        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        total_norm += grad_norm.item()\n        \n        optimizer.step()\n        epoch_loss += loss.item()\n    \n    avg_grad_norm = total_norm / len(dataloader)\n    if avg_grad_norm > 5.0:\n        print(f\"Warning: High gradient norm: {avg_grad_norm:.2f}\")\n    \n    return epoch_loss / len(dataloader)\n\n# --- Improved BiLSTM Model with Better Architecture ---\nclass ImprovedBiLSTMEncoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, dropout):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IDX)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Single bidirectional LSTM layer for simplicity\n        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers=1, bidirectional=True, \n                           dropout=0, batch_first=True)\n        \n        # Layer normalization for stability\n        self.layer_norm = nn.LayerNorm(hid_dim * 2)\n        \n        # Project to decoder size\n        self.fc_hidden = nn.Linear(hid_dim * 2, hid_dim)\n        self.fc_cell = nn.Linear(hid_dim * 2, hid_dim)\n\n    def forward(self, src):\n        # Get actual sequence lengths for packing\n        src_lengths = (src != PAD_IDX).sum(dim=1).cpu()\n        \n        embedded = self.dropout(self.embedding(src))\n        \n        # Pack sequences for efficiency\n        packed_embedded = nn.utils.rnn.pack_padded_sequence(\n            embedded, src_lengths, batch_first=True, enforce_sorted=False)\n        \n        packed_outputs, (hidden, cell) = self.lstm(packed_embedded)\n        \n        # Unpack sequences\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)\n        \n        # Apply layer normalization\n        outputs = self.layer_norm(outputs)\n        \n        # Process final states - hidden/cell are [2, batch, hid_dim] for bidirectional\n        # Concatenate forward and backward final states\n        final_hidden = torch.cat((hidden[0], hidden[1]), dim=1)  # [batch, hid_dim*2]\n        final_cell = torch.cat((cell[0], cell[1]), dim=1)        # [batch, hid_dim*2]\n        \n        # Project to decoder dimensions\n        decoder_hidden = torch.tanh(self.fc_hidden(final_hidden)).unsqueeze(0)  # [1, batch, hid_dim]\n        decoder_cell = torch.tanh(self.fc_cell(final_cell)).unsqueeze(0)        # [1, batch, hid_dim]\n        \n        return outputs, (decoder_hidden, decoder_cell)\n\nclass ImprovedLSTMAttention(nn.Module):\n    def __init__(self, hid_dim):\n        super().__init__()\n        self.hid_dim = hid_dim\n        \n        # Attention mechanism\n        self.W_h = nn.Linear(hid_dim, hid_dim, bias=False)  # For hidden state\n        self.W_s = nn.Linear(hid_dim * 2, hid_dim, bias=False)  # For encoder outputs\n        self.v = nn.Linear(hid_dim, 1, bias=False)\n        \n        self.dropout = nn.Dropout(0.1)\n\n    def forward(self, hidden_cell, encoder_outputs):\n        # Extract hidden state\n        if isinstance(hidden_cell, tuple):\n            hidden = hidden_cell[0]  # [1, batch, hid_dim]\n        else:\n            hidden = hidden_cell\n        \n        # Remove layer dimension if present\n        if hidden.dim() == 3:\n            hidden = hidden.squeeze(0)  # [batch, hid_dim]\n        \n        batch_size, src_len, enc_hid_dim = encoder_outputs.size()\n        \n        # Prepare hidden state for attention computation\n        hidden_transformed = self.W_h(hidden)  # [batch, hid_dim]\n        hidden_expanded = hidden_transformed.unsqueeze(1).expand(-1, src_len, -1)  # [batch, src_len, hid_dim]\n        \n        # Transform encoder outputs\n        encoder_transformed = self.W_s(encoder_outputs)  # [batch, src_len, hid_dim]\n        \n        # Compute attention scores\n        energy = torch.tanh(hidden_expanded + encoder_transformed)  # [batch, src_len, hid_dim]\n        attention_scores = self.v(energy).squeeze(2)  # [batch, src_len]\n        \n        # Create mask for padding tokens\n        mask = (encoder_outputs.sum(dim=2) != 0)  # [batch, src_len]\n        attention_scores = attention_scores.masked_fill(~mask, -1e10)\n        \n        # Apply softmax\n        attention_weights = torch.softmax(attention_scores, dim=1)  # [batch, src_len]\n        attention_weights = self.dropout(attention_weights)\n        \n        return attention_weights\n\nclass ImprovedLSTMDecoder(nn.Module):\n    def __init__(self, vocab_size, emb_dim, hid_dim, dropout, attention):\n        super().__init__()\n        self.attention = attention\n        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=PAD_IDX)\n        self.dropout = nn.Dropout(dropout)\n        \n        # Single layer LSTM\n        self.lstm = nn.LSTM(emb_dim + hid_dim * 2, hid_dim, num_layers=1, batch_first=True)\n        \n        # Output projection with residual connection\n        self.fc_out = nn.Linear(hid_dim + hid_dim * 2 + emb_dim, vocab_size)\n        \n        # Layer normalization\n        self.layer_norm = nn.LayerNorm(hid_dim)\n\n    def forward(self, input, hidden_cell, encoder_outputs):\n        input = input.unsqueeze(1)  # [batch, 1]\n        embedded = self.dropout(self.embedding(input))  # [batch, 1, emb_dim]\n        \n        # Compute attention\n        attention_weights = self.attention(hidden_cell, encoder_outputs)  # [batch, src_len]\n        context = torch.bmm(attention_weights.unsqueeze(1), encoder_outputs)  # [batch, 1, hid_dim*2]\n        \n        # Combine embedding and context\n        rnn_input = torch.cat((embedded, context), dim=2)  # [batch, 1, emb_dim + hid_dim*2]\n        \n        # LSTM forward pass\n        output, new_hidden_cell = self.lstm(rnn_input, hidden_cell)  # [batch, 1, hid_dim]\n        \n        # Apply layer normalization\n        output = self.layer_norm(output)\n        \n        # Prepare for output projection\n        embedded_flat = embedded.squeeze(1)  # [batch, emb_dim]\n        output_flat = output.squeeze(1)      # [batch, hid_dim]\n        context_flat = context.squeeze(1)    # [batch, hid_dim*2]\n        \n        # Final output projection\n        prediction = self.fc_out(torch.cat((output_flat, context_flat, embedded_flat), dim=1))\n        \n        return prediction, new_hidden_cell\n\nclass ImprovedBiLSTMSeq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size = trg.shape[0]\n        trg_len = trg.shape[1]\n        trg_vocab_size = self.decoder.fc_out.out_features\n        \n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden_cell = self.encoder(src)\n        \n        input_token = trg[:, 0]\n        \n        for t in range(1, trg_len):\n            output, hidden_cell = self.decoder(input_token, hidden_cell, encoder_outputs)\n            outputs[:, t] = output\n            \n            # Teacher forcing with scheduled sampling\n            if random.random() < teacher_forcing_ratio:\n                input_token = trg[:, t]\n            else:\n                input_token = output.argmax(1)\n            \n        return outputs\n\n# --- Hyperparameters and Model Instantiation ---\nEMB_DIM = 256\nHID_DIM = 256  # Increased hidden size\nENC_DROPOUT = 0.3\nDEC_DROPOUT = 0.3\nCLIP = 1.0\nNUM_EPOCHS = 60\nPATIENCE = 20\n\nprint(\"\\n=== Creating Improved BiLSTM with Attention Model ===\")\nattn = ImprovedLSTMAttention(HID_DIM)\nenc = ImprovedBiLSTMEncoder(VOCAB_SIZE, EMB_DIM, HID_DIM, ENC_DROPOUT)\ndec = ImprovedLSTMDecoder(VOCAB_SIZE, EMB_DIM, HID_DIM, DEC_DROPOUT, attn)\nmodel = ImprovedBiLSTMSeq2Seq(enc, dec, device).to(device)\n\n# Better initialization\ndef improved_init_weights(m):\n    if isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n        if m.bias is not None:\n            nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Embedding):\n        nn.init.uniform_(m.weight, -0.1, 0.1)\n        # Zero out padding token embedding\n        with torch.no_grad():\n            m.weight[PAD_IDX].fill_(0)\n    elif isinstance(m, nn.LSTM):\n        for name, param in m.named_parameters():\n            if 'weight_ih' in name:\n                nn.init.xavier_uniform_(param.data)\n            elif 'weight_hh' in name:\n                nn.init.orthogonal_(param.data)\n            elif 'bias' in name:\n                nn.init.zeros_(param.data)\n                # Set forget gate bias to 1\n                n = param.size(0)\n                param.data[n//4:n//2].fill_(1.)\n\nmodel.apply(improved_init_weights)\n\n# Better optimizer with lower learning rate\noptimizer = optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5, \n                       betas=(0.9, 0.98), eps=1e-9)\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX, label_smoothing=0.1)\n\n# Warmup + Cosine scheduler\nfrom torch.optim.lr_scheduler import LambdaLR\nimport math\n\ndef get_lr_scheduler(optimizer, num_epochs, warmup_epochs=5):\n    def lr_lambda(epoch):\n        if epoch < warmup_epochs:\n            return float(epoch) / float(max(1, warmup_epochs))\n        else:\n            progress = (epoch - warmup_epochs) / (num_epochs - warmup_epochs)\n            return 0.5 * (1 + math.cos(math.pi * progress))\n    return LambdaLR(optimizer, lr_lambda)\n\nscheduler = get_lr_scheduler(optimizer, NUM_EPOCHS)\n\n# --- Training Loop ---\nbest_bleu = -1\nbest_val_loss = float('inf')\nepochs_no_improve = 0\nepochs_no_loss_improve = 0\n\nprint(\"Starting improved BiLSTM training...\")\nprint(f\"Improved model has {count_parameters(model):,} trainable parameters\")\n\nfor epoch in range(1, NUM_EPOCHS + 1):\n    # More gradual teacher forcing schedule\n    if epoch <= 15:\n        tf_ratio = 0.9\n    elif epoch <= 30:\n        tf_ratio = 0.7\n    elif epoch <= 45:\n        tf_ratio = 0.5\n    else:\n        tf_ratio = 0.3\n    \n    train_loss = train_epoch_with_tf(model, train_loader, optimizer, criterion, CLIP, tf_ratio)\n    valid_loss, valid_bleu = evaluate(model, val_loader, criterion)\n    \n    scheduler.step()\n    \n    # Track best model based on BLEU\n    if valid_bleu > best_bleu:\n        best_bleu = valid_bleu\n        epochs_no_improve = 0\n        torch.save({\n            'epoch': epoch,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'best_bleu': best_bleu,\n            'valid_loss': valid_loss\n        }, 'improved-bilstm-model.pt')\n        print(f\"*** New best BLEU: {valid_bleu:.2f} - Model saved! ***\")\n    else:\n        epochs_no_improve += 1\n    \n    # Track validation loss for learning rate adjustment\n    if valid_loss < best_val_loss:\n        best_val_loss = valid_loss\n        epochs_no_loss_improve = 0\n    else:\n        epochs_no_loss_improve += 1\n    \n    # Reduce LR if validation loss plateaus\n    if epochs_no_loss_improve >= 8:\n        for param_group in optimizer.param_groups:\n            old_lr = param_group['lr']\n            param_group['lr'] *= 0.8\n            print(f\"Reduced LR from {old_lr:.6f} to {param_group['lr']:.6f}\")\n        epochs_no_loss_improve = 0\n    \n    current_lr = optimizer.param_groups[0]['lr']\n    print(f'Epoch: {epoch:02} | Train Loss: {train_loss:.3f} | Val Loss: {valid_loss:.3f} | Val BLEU: {valid_bleu:.2f} | TF: {tf_ratio:.2f} | LR: {current_lr:.6f}')\n    \n    # Early stopping\n    if epochs_no_improve >= PATIENCE:\n        print(f'BLEU not improving for {PATIENCE} epochs - early stopping!')\n        break\n    \n    # Stop if learning rate becomes too small\n    if current_lr < 1e-7:\n        print(\"Learning rate too small - stopping!\")\n        break\n\nprint(f\"Training completed! Best BLEU score: {best_bleu:.2f}\")\n\n# Improved translation with better decoding\ndef improved_translate_sentence(model, sentence, max_len=40):\n    model.eval()\n    tokens = [SOS_IDX] + sp.encode_as_ids(sentence.lower().strip()) + [EOS_IDX]\n    src_tensor = torch.LongTensor(tokens).unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        encoder_outputs, hidden_cell = model.encoder(src_tensor)\n    \n    trg_indexes = [SOS_IDX]\n    \n    for i in range(max_len):\n        trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n        \n        with torch.no_grad():\n            output, hidden_cell = model.decoder(trg_tensor, hidden_cell, encoder_outputs)\n        \n        # Apply repetition penalty\n        if len(trg_indexes) > 1:\n            for prev_token in set(trg_indexes[1:]):  # Exclude SOS\n                if prev_token < output.size(1):\n                    count = trg_indexes.count(prev_token)\n                    penalty = min(count * 1.2, 3.0)  # Cap penalty\n                    output[0, prev_token] -= penalty\n        \n        # Use nucleus sampling (top-p)\n        sorted_logits, sorted_indices = torch.sort(output, descending=True)\n        cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)\n        \n        # Remove tokens with cumulative probability above the threshold (nucleus)\n        sorted_indices_to_remove = cumulative_probs > 0.9\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n        \n        # Set logits to -inf for removed tokens\n        sorted_logits[sorted_indices_to_remove] = float('-inf')\n        \n        # Sample from the filtered distribution\n        probs = torch.softmax(sorted_logits / 0.8, dim=-1)  # Temperature scaling\n        pred_token = torch.multinomial(probs, 1)[0, 0].item()\n        pred_token = sorted_indices[0, pred_token].item()\n        \n        trg_indexes.append(pred_token)\n        if pred_token == EOS_IDX:\n            break\n    \n    return sp.decode(trg_indexes)\n\n# Load best model\ntry:\n    checkpoint = torch.load('improved-bilstm-model.pt', map_location=device)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"Improved BiLSTM model loaded! Best BLEU: {checkpoint['best_bleu']:.2f}\")\nexcept FileNotFoundError:\n    print(\"Using current trained model state...\")\n\nsample_sentences = [\n    \"মই ঘৰত থাকোঁ।\",\n    \"তেওঁ আজি বিদ্যালয়লৈ গ'ল।\",\n    \"বইখন টেবুলৰ ওপৰত আছে।\"\n]\n\nprint(\"\\n=== Improved BiLSTM Translation Examples ===\")\nfor sentence in sample_sentences:\n    greedy_translation = improved_translate_sentence(model, sentence)\n    beam_translation = bilstm_beam_search(model, sentence, beam_width=3, max_len=40)\n    print(f\"SOURCE:     {sentence}\")\n    print(f\"IMPROVED:   {greedy_translation}\")\n    print(f\"BEAM (k=3): {beam_translation}\")\n    print(\"-\" * 50)\n\nprint(\"\\n=== Final Evaluation ===\")\ntest_loss, test_bleu = evaluate(model, test_loader, criterion)\nprint(f'Test Loss: {test_loss:.3f} | Test BLEU: {test_bleu:.2f}')\n\n# Additional evaluation with more samples\nprint(\"\\n=== Additional Improved BiLSTM Test Sentences ===\")\nadditional_test_sentences = [\n    \"আমি খাওয়া-দাওয়া কৰি আছো।\",\n    \"তেওঁ কিতাপ পঢ়ি আছে।\", \n    \"আজি বৰষুণ হৈছে।\",\n    \"স্কুলত পাঠদান চলি আছে।\",\n    \"আমাৰ ঘৰখন ডাঙৰ।\"\n]\n\nfor sentence in additional_test_sentences:\n    try:\n        greedy_translation = improved_translate_sentence(model, sentence)\n        beam_translation = bilstm_beam_search(model, sentence, beam_width=3, max_len=40)\n        print(f\"SOURCE:     {sentence}\")\n        print(f\"IMPROVED:   {greedy_translation}\")\n        print(f\"BEAM (k=3): {beam_translation}\")\n        print(\"-\" * 30)\n    except Exception as e:\n        print(f\"Error translating '{sentence}': {str(e)}\")\n        continue","metadata":{"_uuid":"f6b52c72-d0fe-48b7-a8d3-afaf8862f1a4","_cell_guid":"f8f1b2c9-e823-406d-a1f3-3207b01659e4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false},"execution":{"iopub.status.busy":"2025-07-20T09:15:38.143364Z","iopub.execute_input":"2025-07-20T09:15:38.144011Z","iopub.status.idle":"2025-07-20T10:03:53.147600Z","shell.execute_reply.started":"2025-07-20T09:15:38.143983Z","shell.execute_reply":"2025-07-20T10:03:53.146787Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n\n=== VOCABULARY INFORMATION ===\nVocabulary size: 8000\nPAD token: 0 -> '<unk>'\nSOS token: 1 -> '<s>'\nEOS token: 2 -> '</s>'\nUNK token: 0 -> '<unk>'\n\nSample vocabulary (first 50 tokens):\n  0: '<unk>'\n  1: '<s>'\n  2: '</s>'\n  3: '▁t'\n  4: 'he'\n  5: '▁a'\n  6: 'in'\n  7: '▁the'\n  8: '▁ক'\n  9: 'য়'\n 10: 'াৰ'\n 11: '▁ব'\n 12: '▁প'\n 13: '▁o'\n 14: '▁s'\n 15: '▁স'\n 16: 're'\n 17: '্ৰ'\n 18: '▁b'\n 19: 'er'\n 20: 'ha'\n 21: '▁c'\n 22: 'on'\n 23: 'en'\n 24: '▁আ'\n 25: '▁p'\n 26: '▁w'\n 27: 'is'\n 28: '▁in'\n 29: '্য'\n 30: 'ed'\n 31: 'ৰা'\n 32: '▁m'\n 33: '▁of'\n 34: 'ar'\n 35: 'an'\n 36: '্ত'\n 37: 'at'\n 38: 'ৰি'\n 39: '▁ম'\n 40: '▁f'\n 41: '▁d'\n 42: 'it'\n 43: '▁অ'\n 44: 'or'\n 45: '▁ন'\n 46: '▁হ'\n 47: 'ান'\n 48: '▁দ'\n 49: 'al'\n\nHigh-frequency tokens (500-600):\n500: '▁গ্ৰ'\n501: 'ুক'\n502: '▁day'\n503: '▁tak'\n504: 'ord'\n505: 'hile'\n506: 'ight'\n507: '▁বিজ'\n508: 'ঘট'\n509: '▁বৃ'\n510: 'হণ'\n511: 'ৎস'\n512: 'og'\n513: '▁ও'\n514: '▁থকা'\n515: '▁মন্ত'\n516: 'ire'\n517: '▁part'\n518: '▁।'\n519: '▁comm'\n520: '▁ভাৰত'\n521: 'হা'\n522: 'out'\n523: 'হত'\n524: '▁দল'\n525: '▁3'\n526: 'cted'\n527: '▁ch'\n528: 'ear'\n529: '▁dep'\n530: '▁ma'\n531: 'ther'\n532: '▁মুখ্য'\n533: '▁all'\n534: '▁cou'\n535: '▁আছে'\n536: 'hief'\n537: '▁ব্যৱ'\n538: '▁there'\n539: 'োধ'\n540: '▁or'\n541: '▁sh'\n542: 'qu'\n543: '▁সে'\n544: 'য়াৰ'\n545: '▁নাম'\n546: '▁স্থ'\n547: 'গৰাকী'\n548: '্ল'\n549: 'িছে'\n550: '▁chief'\n551: '▁উল্লে'\n552: 'িম'\n553: '▁৩'\n554: '▁লা'\n555: 'nder'\n556: 'েট'\n557: '▁ঘট'\n558: 'ঞ্চল'\n559: '▁cor'\n560: 'rou'\n561: '▁শিক্ষ'\n562: 'াত্ৰ'\n563: 'ub'\n564: 'ath'\n565: '্বৰ'\n566: 'নী'\n567: '▁but'\n568: '▁again'\n569: 'োচ'\n570: '▁সভ'\n571: '▁মৃত'\n572: 'ost'\n573: '▁আম'\n574: 'fe'\n575: 'প্ৰ'\n576: '▁time'\n577: '▁two'\n578: 'one'\n579: '▁under'\n580: 'দান'\n581: 'দেশ'\n582: '▁কো'\n583: '▁চিক'\n584: '▁সেই'\n585: '▁pres'\n586: '▁মাজ'\n587: '00'\n588: '▁stud'\n589: 'িচ'\n590: '▁সম্প'\n591: 'দ্যালয়'\n592: '▁out'\n593: '▁জান'\n594: '▁কাৰ্য'\n595: '▁other'\n596: '▁gu'\n597: 'll'\n598: 'কাশ'\n599: '▁ele'\n\n=== TOKENIZATION EXAMPLES ===\n\nSentence: মই ঘৰত থাকোঁ।\nTokens: ['▁মই', '▁ঘৰত', '▁থাক', 'োঁ', '।']\nToken IDs: [2697, 3650, 756, 1675, 7872]\nReconstructed: মই ঘৰত থাকোঁ।\n\nSentence: তেওঁ আজি বিদ্যালয়লৈ গ'ল।\nTokens: ['▁তেওঁ', '▁আজি', '▁বিদ্যালয়', 'লৈ', '▁গ', \"'\", 'ল', '।']\nToken IDs: [295, 816, 1246, 146, 72, 7896, 7839, 7872]\nReconstructed: তেওঁ আজি বিদ্যালয়লৈ গ'ল।\n\nSentence: বইখন টেবুলৰ ওপৰত আছে।\nTokens: ['▁ব', 'ই', 'খন', '▁ট', 'েব', 'ুলৰ', '▁ওপৰত', '▁আছে', '।']\nToken IDs: [11, 7862, 249, 250, 1720, 3544, 1709, 535, 7872]\nReconstructed: বইখন টেবুলৰ ওপৰত আছে।\n\nSentence: I am at home.\nTokens: ['▁i', '▁am', '▁at', '▁home', '.']\nToken IDs: [1135, 471, 147, 1418, 7861]\nReconstructed: i am at home.\n\nSentence: He went to school today.\nTokens: ['▁he', '▁went', '▁to', '▁school', '▁today', '.']\nToken IDs: [152, 2714, 53, 901, 775, 7861]\nReconstructed: he went to school today.\n\nSentence: The book is on the table.\nTokens: ['▁the', '▁book', '▁is', '▁on', '▁the', '▁t', 'able', '.']\nToken IDs: [7, 2440, 123, 99, 7, 3, 1256, 7861]\nReconstructed: the book is on the table.\n\n=== VOCABULARY ANALYSIS ===\nTotal unique tokens in training: 7793\nAssamese tokens: 4548\nEnglish tokens: 7474\nVocabulary coverage: 97.4%\n\nTop 20 most frequent tokens:\n'▁the' (ID: 7): 10682 times\n',' (ID: 7857): 8371 times\n'.' (ID: 7861): 6697 times\n'▁of' (ID: 33): 4551 times\n'।' (ID: 7872): 4468 times\n'▁in' (ID: 28): 3608 times\n'▁to' (ID: 53): 2788 times\n'▁a' (ID: 5): 2433 times\n'-' (ID: 7884): 2026 times\n's' (ID: 7826): 1958 times\n'▁and' (ID: 94): 1840 times\n'ৰ' (ID: 7822): 1757 times\n'▁on' (ID: 99): 1392 times\n'▁is' (ID: 123): 1327 times\n'’' (ID: 7894): 1257 times\n'ত' (ID: 7832): 1172 times\n''' (ID: 7896): 1130 times\n'▁আৰু' (ID: 150): 1101 times\n'▁এই' (ID: 139): 1076 times\n'ক' (ID: 7831): 1068 times\n\n=== LANGUAGE DISTRIBUTION IN VOCABULARY ===\nTotal vocabulary size: 8000\nTokens used in Assamese: 4548\nTokens used in English: 7474\nShared tokens: 4229\nAssamese-only tokens: 319\nEnglish-only tokens: 3245\n\nTop 10 shared tokens:\n  '▁the': 10682 times (Asm: 6, Eng: 10676)\n  ',': 8371 times (Asm: 1767, Eng: 6604)\n  '.': 6697 times (Asm: 161, Eng: 6536)\n  '▁of': 4551 times (Asm: 5, Eng: 4546)\n  '।': 4468 times (Asm: 3553, Eng: 915)\n  '▁in': 3608 times (Asm: 4, Eng: 3604)\n  '▁to': 2788 times (Asm: 3, Eng: 2785)\n  '▁a': 2433 times (Asm: 6, Eng: 2427)\n  '-': 2026 times (Asm: 726, Eng: 1300)\n  's': 1958 times (Asm: 19, Eng: 1939)\n\nTop 10 Assamese-only tokens:\n  '▁কিয়': 28 times\n  '▁পোহৰলৈ': 26 times\n  '▁নেকি': 24 times\n  'খনে': 22 times\n  '▁ৱাৰ্ড': 21 times\n  '▁জম্মু': 21 times\n  'ান্ধ': 20 times\n  'গে': 19 times\n  '▁ৰাষ্ট্ৰপতি': 18 times\n  '▁চোৰ': 18 times\n\nTop 10 English-only tokens:\n  '▁that': 994 times\n  '▁has': 989 times\n  '▁from': 728 times\n  '▁have': 691 times\n  '▁been': 644 times\n  '▁also': 531 times\n  '▁police': 526 times\n  '▁it': 512 times\n  '▁people': 500 times\n  '▁are': 492 times\n\n=== Creating Improved BiLSTM with Attention Model ===\nStarting improved BiLSTM training...\nImproved model has 14,860,352 trainable parameters\n*** New best BLEU: 0.00 - Model saved! ***\nEpoch: 01 | Train Loss: 9.030 | Val Loss: 9.027 | Val BLEU: 0.00 | TF: 0.90 | LR: 0.000100\n*** New best BLEU: 0.01 - Model saved! ***\nEpoch: 02 | Train Loss: 7.270 | Val Loss: 6.927 | Val BLEU: 0.01 | TF: 0.90 | LR: 0.000200\n*** New best BLEU: 0.02 - Model saved! ***\nEpoch: 03 | Train Loss: 6.859 | Val Loss: 6.971 | Val BLEU: 0.02 | TF: 0.90 | LR: 0.000300\n*** New best BLEU: 0.13 - Model saved! ***\nEpoch: 04 | Train Loss: 6.655 | Val Loss: 7.003 | Val BLEU: 0.13 | TF: 0.90 | LR: 0.000400\n*** New best BLEU: 0.50 - Model saved! ***\nEpoch: 05 | Train Loss: 6.265 | Val Loss: 7.248 | Val BLEU: 0.50 | TF: 0.90 | LR: 0.000500\n*** New best BLEU: 1.21 - Model saved! ***\nEpoch: 06 | Train Loss: 5.810 | Val Loss: 7.206 | Val BLEU: 1.21 | TF: 0.90 | LR: 0.000500\n*** New best BLEU: 1.63 - Model saved! ***\nEpoch: 07 | Train Loss: 5.379 | Val Loss: 7.233 | Val BLEU: 1.63 | TF: 0.90 | LR: 0.000498\n*** New best BLEU: 2.12 - Model saved! ***\nEpoch: 08 | Train Loss: 5.002 | Val Loss: 7.259 | Val BLEU: 2.12 | TF: 0.90 | LR: 0.000496\n*** New best BLEU: 2.27 - Model saved! ***\nEpoch: 09 | Train Loss: 4.697 | Val Loss: 7.371 | Val BLEU: 2.27 | TF: 0.90 | LR: 0.000494\n*** New best BLEU: 2.53 - Model saved! ***\nReduced LR from 0.000490 to 0.000392\nEpoch: 10 | Train Loss: 4.430 | Val Loss: 7.448 | Val BLEU: 2.53 | TF: 0.90 | LR: 0.000392\nEpoch: 11 | Train Loss: 4.156 | Val Loss: 7.586 | Val BLEU: 2.21 | TF: 0.90 | LR: 0.000485\nEpoch: 12 | Train Loss: 3.993 | Val Loss: 7.630 | Val BLEU: 2.37 | TF: 0.90 | LR: 0.000480\n*** New best BLEU: 2.54 - Model saved! ***\nEpoch: 13 | Train Loss: 3.832 | Val Loss: 7.678 | Val BLEU: 2.54 | TF: 0.90 | LR: 0.000474\nEpoch: 14 | Train Loss: 3.661 | Val Loss: 7.799 | Val BLEU: 2.51 | TF: 0.90 | LR: 0.000468\n*** New best BLEU: 2.83 - Model saved! ***\nEpoch: 15 | Train Loss: 3.503 | Val Loss: 7.939 | Val BLEU: 2.83 | TF: 0.90 | LR: 0.000460\n*** New best BLEU: 2.91 - Model saved! ***\nEpoch: 16 | Train Loss: 3.685 | Val Loss: 7.604 | Val BLEU: 2.91 | TF: 0.70 | LR: 0.000452\nEpoch: 17 | Train Loss: 3.581 | Val Loss: 7.642 | Val BLEU: 2.69 | TF: 0.70 | LR: 0.000444\nReduced LR from 0.000434 to 0.000347\nEpoch: 18 | Train Loss: 3.490 | Val Loss: 7.654 | Val BLEU: 2.78 | TF: 0.70 | LR: 0.000347\nEpoch: 19 | Train Loss: 3.358 | Val Loss: 7.741 | Val BLEU: 2.83 | TF: 0.70 | LR: 0.000424\nEpoch: 20 | Train Loss: 3.294 | Val Loss: 7.743 | Val BLEU: 2.89 | TF: 0.70 | LR: 0.000414\n*** New best BLEU: 3.19 - Model saved! ***\nEpoch: 21 | Train Loss: 3.220 | Val Loss: 7.824 | Val BLEU: 3.19 | TF: 0.70 | LR: 0.000403\n*** New best BLEU: 3.23 - Model saved! ***\nEpoch: 22 | Train Loss: 3.134 | Val Loss: 7.892 | Val BLEU: 3.23 | TF: 0.70 | LR: 0.000391\nEpoch: 23 | Train Loss: 3.078 | Val Loss: 7.887 | Val BLEU: 3.17 | TF: 0.70 | LR: 0.000379\n*** New best BLEU: 3.28 - Model saved! ***\nEpoch: 24 | Train Loss: 3.024 | Val Loss: 7.931 | Val BLEU: 3.28 | TF: 0.70 | LR: 0.000367\nEpoch: 25 | Train Loss: 2.936 | Val Loss: 7.997 | Val BLEU: 3.27 | TF: 0.70 | LR: 0.000354\nReduced LR from 0.000341 to 0.000273\nEpoch: 26 | Train Loss: 2.871 | Val Loss: 8.043 | Val BLEU: 2.88 | TF: 0.70 | LR: 0.000273\nEpoch: 27 | Train Loss: 2.796 | Val Loss: 8.083 | Val BLEU: 3.12 | TF: 0.70 | LR: 0.000327\nEpoch: 28 | Train Loss: 2.762 | Val Loss: 8.080 | Val BLEU: 3.13 | TF: 0.70 | LR: 0.000314\n*** New best BLEU: 3.42 - Model saved! ***\nEpoch: 29 | Train Loss: 2.702 | Val Loss: 8.143 | Val BLEU: 3.42 | TF: 0.70 | LR: 0.000300\n*** New best BLEU: 3.44 - Model saved! ***\nEpoch: 30 | Train Loss: 2.632 | Val Loss: 8.193 | Val BLEU: 3.44 | TF: 0.70 | LR: 0.000286\nEpoch: 31 | Train Loss: 2.900 | Val Loss: 7.993 | Val BLEU: 2.92 | TF: 0.50 | LR: 0.000271\nEpoch: 32 | Train Loss: 2.868 | Val Loss: 7.976 | Val BLEU: 3.28 | TF: 0.50 | LR: 0.000257\nEpoch: 33 | Train Loss: 2.838 | Val Loss: 7.965 | Val BLEU: 2.91 | TF: 0.50 | LR: 0.000243\nReduced LR from 0.000229 to 0.000183\nEpoch: 34 | Train Loss: 2.768 | Val Loss: 7.971 | Val BLEU: 3.29 | TF: 0.50 | LR: 0.000183\nEpoch: 35 | Train Loss: 2.723 | Val Loss: 7.982 | Val BLEU: 3.13 | TF: 0.50 | LR: 0.000214\nEpoch: 36 | Train Loss: 2.686 | Val Loss: 8.021 | Val BLEU: 3.19 | TF: 0.50 | LR: 0.000200\nEpoch: 37 | Train Loss: 2.625 | Val Loss: 8.059 | Val BLEU: 3.07 | TF: 0.50 | LR: 0.000186\nEpoch: 38 | Train Loss: 2.606 | Val Loss: 8.043 | Val BLEU: 3.25 | TF: 0.50 | LR: 0.000173\nEpoch: 39 | Train Loss: 2.584 | Val Loss: 8.065 | Val BLEU: 3.33 | TF: 0.50 | LR: 0.000159\nEpoch: 40 | Train Loss: 2.521 | Val Loss: 8.108 | Val BLEU: 3.26 | TF: 0.50 | LR: 0.000146\nEpoch: 41 | Train Loss: 2.500 | Val Loss: 8.117 | Val BLEU: 3.41 | TF: 0.50 | LR: 0.000133\nReduced LR from 0.000121 to 0.000097\nEpoch: 42 | Train Loss: 2.487 | Val Loss: 8.154 | Val BLEU: 3.15 | TF: 0.50 | LR: 0.000097\n*** New best BLEU: 3.55 - Model saved! ***\nEpoch: 43 | Train Loss: 2.427 | Val Loss: 8.171 | Val BLEU: 3.55 | TF: 0.50 | LR: 0.000109\nEpoch: 44 | Train Loss: 2.412 | Val Loss: 8.141 | Val BLEU: 3.35 | TF: 0.50 | LR: 0.000097\nEpoch: 45 | Train Loss: 2.385 | Val Loss: 8.192 | Val BLEU: 3.16 | TF: 0.50 | LR: 0.000086\nEpoch: 46 | Train Loss: 2.708 | Val Loss: 8.049 | Val BLEU: 3.12 | TF: 0.30 | LR: 0.000076\nEpoch: 47 | Train Loss: 2.717 | Val Loss: 8.002 | Val BLEU: 3.24 | TF: 0.30 | LR: 0.000066\nEpoch: 48 | Train Loss: 2.678 | Val Loss: 8.014 | Val BLEU: 3.08 | TF: 0.30 | LR: 0.000056\nEpoch: 49 | Train Loss: 2.642 | Val Loss: 8.027 | Val BLEU: 3.08 | TF: 0.30 | LR: 0.000048\nReduced LR from 0.000040 to 0.000032\nEpoch: 50 | Train Loss: 2.632 | Val Loss: 8.019 | Val BLEU: 2.99 | TF: 0.30 | LR: 0.000032\nEpoch: 51 | Train Loss: 2.622 | Val Loss: 8.011 | Val BLEU: 3.03 | TF: 0.30 | LR: 0.000032\nEpoch: 52 | Train Loss: 2.595 | Val Loss: 8.001 | Val BLEU: 3.02 | TF: 0.30 | LR: 0.000026\nEpoch: 53 | Train Loss: 2.613 | Val Loss: 8.003 | Val BLEU: 2.92 | TF: 0.30 | LR: 0.000020\nEpoch: 54 | Train Loss: 2.584 | Val Loss: 8.013 | Val BLEU: 2.95 | TF: 0.30 | LR: 0.000015\nEpoch: 55 | Train Loss: 2.576 | Val Loss: 8.009 | Val BLEU: 3.01 | TF: 0.30 | LR: 0.000010\nEpoch: 56 | Train Loss: 2.581 | Val Loss: 8.008 | Val BLEU: 2.96 | TF: 0.30 | LR: 0.000006\nEpoch: 57 | Train Loss: 2.578 | Val Loss: 8.009 | Val BLEU: 2.87 | TF: 0.30 | LR: 0.000004\nReduced LR from 0.000002 to 0.000001\nEpoch: 58 | Train Loss: 2.577 | Val Loss: 8.000 | Val BLEU: 2.94 | TF: 0.30 | LR: 0.000001\nEpoch: 59 | Train Loss: 2.568 | Val Loss: 7.997 | Val BLEU: 2.94 | TF: 0.30 | LR: 0.000000\nEpoch: 60 | Train Loss: 2.568 | Val Loss: 8.000 | Val BLEU: 2.95 | TF: 0.30 | LR: 0.000000\nLearning rate too small - stopping!\nTraining completed! Best BLEU score: 3.55\nImproved BiLSTM model loaded! Best BLEU: 3.55\n\n=== Improved BiLSTM Translation Examples ===\nSOURCE:     মই ঘৰত থাকোঁ।\nIMPROVED:   i am is at home.\nBEAM (k=3): i will be at home.\n--------------------------------------------------\nSOURCE:     তেওঁ আজি বিদ্যালয়লৈ গ'ল।\nIMPROVED:   he goes to ganga the today.\nBEAM (k=3): today, the school is going today.\n--------------------------------------------------\nSOURCE:     বইখন টেবুলৰ ওপৰত আছে।\nIMPROVED:   the student is on theৰাধ.\nBEAM (k=3): the photo is on the photo.\n--------------------------------------------------\n\n=== Final Evaluation ===\nTest Loss: 8.174 | Test BLEU: 3.42\n\n=== Additional Improved BiLSTM Test Sentences ===\nSOURCE:     আমি খাওয়া-দাওয়া কৰি আছো।\nIMPROVED:   web great friends, being in theone.\nBEAM (k=3): we have also been closed in the by the bado.\n------------------------------\nSOURCE:     তেওঁ কিতাপ পঢ়ি আছে।\nIMPROVED:   he has to aut the.\nBEAM (k=3): he has he has to b.\n------------------------------\nSOURCE:     আজি বৰষুণ হৈছে।\nIMPROVED:   today is today.\nBEAM (k=3): today is today.\n------------------------------\nSOURCE:     স্কুলত পাঠদান চলি আছে।\nIMPROVED:   there is underway in the.\nBEAM (k=3): there is underway in the.\n------------------------------\nSOURCE:     আমাৰ ঘৰখন ডাঙৰ।\nIMPROVED:   we are in our house.\nBEAM (k=3): our house is in our house.\n------------------------------\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Add this function after the analyze_vocabulary function:\ndef analyze_language_distribution(sp_model, train_df):\n    \"\"\"Analyze how vocabulary is distributed between languages\"\"\"\n    print(f\"\\n=== LANGUAGE DISTRIBUTION IN VOCABULARY ===\")\n    \n    from collections import Counter, defaultdict\n    \n    # Count token frequency by language\n    asm_token_counts = Counter()\n    eng_token_counts = Counter()\n    \n    # Analyze Assamese sentences\n    for text in train_df['asm'].astype(str):\n        tokens = sp_model.encode_as_pieces(text.lower().strip())\n        asm_token_counts.update(tokens)\n    \n    # Analyze English sentences  \n    for text in train_df['eng'].astype(str):\n        tokens = sp_model.encode_as_pieces(text.lower().strip())\n        eng_token_counts.update(tokens)\n    \n    # Find shared vs unique tokens\n    asm_tokens = set(asm_token_counts.keys())\n    eng_tokens = set(eng_token_counts.keys())\n    shared_tokens = asm_tokens & eng_tokens\n    asm_only = asm_tokens - eng_tokens\n    eng_only = eng_tokens - asm_tokens\n    \n    print(f\"Total vocabulary size: {VOCAB_SIZE}\")\n    print(f\"Tokens used in Assamese: {len(asm_tokens)}\")\n    print(f\"Tokens used in English: {len(eng_tokens)}\")\n    print(f\"Shared tokens: {len(shared_tokens)}\")\n    print(f\"Assamese-only tokens: {len(asm_only)}\")\n    print(f\"English-only tokens: {len(eng_only)}\")\n    \n    print(f\"\\nTop 10 shared tokens:\")\n    shared_counts = {token: asm_token_counts[token] + eng_token_counts[token] \n                    for token in shared_tokens}\n    for token, count in sorted(shared_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n        print(f\"  '{token}': {count} times (Asm: {asm_token_counts[token]}, Eng: {eng_token_counts[token]})\")\n    \n    print(f\"\\nTop 10 Assamese-only tokens:\")\n    for token, count in asm_token_counts.most_common(10):\n        if token in asm_only:\n            print(f\"  '{token}': {count} times\")\n    \n    print(f\"\\nTop 10 English-only tokens:\")\n    for token, count in eng_token_counts.most_common(10):\n        if token in eng_only:\n            print(f\"  '{token}': {count} times\")\n\n# Call this function after the existing analyze_vocabulary call:\nanalyze_vocabulary(sp, train_df, val_df)\nanalyze_language_distribution(sp, train_df)  # Add this line","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-19T20:49:15.018092Z","iopub.execute_input":"2025-07-19T20:49:15.018381Z","iopub.status.idle":"2025-07-19T20:49:18.817153Z","shell.execute_reply.started":"2025-07-19T20:49:15.018357Z","shell.execute_reply":"2025-07-19T20:49:18.816367Z"}},"outputs":[{"name":"stdout","text":"\n=== VOCABULARY ANALYSIS ===\nTotal unique tokens in training: 7793\nAssamese tokens: 4548\nEnglish tokens: 7474\nVocabulary coverage: 97.4%\n\nTop 20 most frequent tokens:\n'▁the' (ID: 7): 10682 times\n',' (ID: 7857): 8371 times\n'.' (ID: 7861): 6697 times\n'▁of' (ID: 33): 4551 times\n'।' (ID: 7872): 4468 times\n'▁in' (ID: 28): 3608 times\n'▁to' (ID: 53): 2788 times\n'▁a' (ID: 5): 2433 times\n'-' (ID: 7884): 2026 times\n's' (ID: 7826): 1958 times\n'▁and' (ID: 94): 1840 times\n'ৰ' (ID: 7822): 1757 times\n'▁on' (ID: 99): 1392 times\n'▁is' (ID: 123): 1327 times\n'’' (ID: 7894): 1257 times\n'ত' (ID: 7832): 1172 times\n''' (ID: 7896): 1130 times\n'▁আৰু' (ID: 150): 1101 times\n'▁এই' (ID: 139): 1076 times\n'ক' (ID: 7831): 1068 times\n\n=== LANGUAGE DISTRIBUTION IN VOCABULARY ===\nTotal vocabulary size: 8000\nTokens used in Assamese: 4548\nTokens used in English: 7474\nShared tokens: 4229\nAssamese-only tokens: 319\nEnglish-only tokens: 3245\n\nTop 10 shared tokens:\n  '▁the': 10682 times (Asm: 6, Eng: 10676)\n  ',': 8371 times (Asm: 1767, Eng: 6604)\n  '.': 6697 times (Asm: 161, Eng: 6536)\n  '▁of': 4551 times (Asm: 5, Eng: 4546)\n  '।': 4468 times (Asm: 3553, Eng: 915)\n  '▁in': 3608 times (Asm: 4, Eng: 3604)\n  '▁to': 2788 times (Asm: 3, Eng: 2785)\n  '▁a': 2433 times (Asm: 6, Eng: 2427)\n  '-': 2026 times (Asm: 726, Eng: 1300)\n  's': 1958 times (Asm: 19, Eng: 1939)\n\nTop 10 Assamese-only tokens:\n\nTop 10 English-only tokens:\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}