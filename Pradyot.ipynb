{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KuqBCw8NW46C",
        "outputId": "0aa7ccfd-2b70-47a4-875c-c467228c15be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "smooth = SmoothingFunction().method1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1awbDNkXXJ4p",
        "outputId": "29fb46f2-7097-42bf-9e53-b3379dc76057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total examples: 7949\n",
            "Train size: 6359, Val size: 1590\n"
          ]
        }
      ],
      "source": [
        "# Cell 2: Load Cleaned DataFrame & Train/Validation Split\n",
        "# —————————————————————————————————————————————\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add on_bad_lines='skip' to skip lines with parsing errors\n",
        "df = pd.read_csv(\"pradyot3.tsv\", sep=\"\\t\", names=[\"asm\",\"eng\"], on_bad_lines='skip')\n",
        "print(f\"Total examples: {len(df)}\")\n",
        "\n",
        "# Split using scikit-learn\n",
        "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "print(f\"Train size: {len(train_df)}, Val size: {len(val_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2dDOMioXN5g",
        "outputId": "140fab7a-5686-4501-859c-b8eba1b06de5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assamese vocab size: 8859, English vocab size: 12581\n"
          ]
        }
      ],
      "source": [
        "# Cell 3: Tokenization & Vocabulary Building\n",
        "# —————————————————————————————————————\n",
        "from collections import Counter\n",
        "\n",
        "def tokenize(text):\n",
        "    # Ensure text is a string before stripping\n",
        "    return str(text).strip().split()\n",
        "\n",
        "# Build counters\n",
        "ctr_asm, ctr_eng = Counter(), Counter()\n",
        "# Convert columns to string type to handle potential non-string values (like NaNs)\n",
        "for sent in train_df[\"asm\"].astype(str):\n",
        "    ctr_asm.update(tokenize(sent))\n",
        "for sent in train_df[\"eng\"].astype(str):\n",
        "    ctr_eng.update(tokenize(sent))\n",
        "\n",
        "# Special tokens\n",
        "PAD, SOS, EOS, UNK = \"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"\n",
        "def build_vocab(counter, min_freq=2):\n",
        "    itos = [PAD, SOS, EOS, UNK] + [w for w,c in counter.items() if c >= min_freq]\n",
        "    stoi = {tok:i for i,tok in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "asm2idx, idx2asm = build_vocab(ctr_asm)\n",
        "eng2idx, idx2eng = build_vocab(ctr_eng)\n",
        "\n",
        "print(f\"Assamese vocab size: {len(idx2asm)}, English vocab size: {len(idx2eng)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBHW_9wjXP9Z",
        "outputId": "99eb49e5-4d8d-407e-ba6e-911cd3ba3b46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset validation:\n",
            "Total examples: 7949\n",
            "Train examples: 6359\n",
            "Validation examples: 1590\n",
            "Average Assamese length: 15.0\n",
            "Average English length: 28.4\n",
            "Very short sentences (< 2 tokens): 16\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Data validation and statistics\n",
        "print(\"Dataset validation:\")\n",
        "print(f\"Total examples: {len(df)}\")\n",
        "print(f\"Train examples: {len(train_df)}\")\n",
        "print(f\"Validation examples: {len(val_df)}\")\n",
        "\n",
        "# Check for empty or very short sentences\n",
        "train_lengths = []\n",
        "for _, row in train_df.iterrows():\n",
        "    asm_len = len(tokenize(row['asm']))\n",
        "    eng_len = len(tokenize(row['eng']))\n",
        "    train_lengths.append([asm_len, eng_len])\n",
        "\n",
        "asm_lens, eng_lens = zip(*train_lengths)\n",
        "print(f\"Average Assamese length: {np.mean(asm_lens):.1f}\")\n",
        "print(f\"Average English length: {np.mean(eng_lens):.1f}\")\n",
        "\n",
        "# Check for very short sentences that might cause issues\n",
        "short_sentences = sum(1 for a, e in train_lengths if a < 2 or e < 2)\n",
        "print(f\"Very short sentences (< 2 tokens): {short_sentences}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c_qqTvvXTLq",
        "outputId": "788dc1c6-34ee-427a-92f1-455c7c4a9970"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train dataset size: 6359\n",
            "Validation dataset size: 1590\n"
          ]
        }
      ],
      "source": [
        "# Cell 4: Functional Dataset + Collate fn\n",
        "# —————————————————————\n",
        "\n",
        "# Updated hyperparameters for 10k dataset\n",
        "HYP = {\n",
        "    \"emb_dim\": 256,  # Reduced from 512 to prevent overfitting\n",
        "    \"hid_dim\": 512,\n",
        "    \"n_layers\": 2,\n",
        "    \"dropout\": 0.3,  # Increased dropout for regularization\n",
        "    \"lr\": 1e-3,\n",
        "    \"epochs\": 30,    # Increased epochs for smaller dataset\n",
        "    \"beam_width\": 5,\n",
        "    \"batch_size\": 64,  # Reduced batch size for stability\n",
        "    \"clip_grad\": 1.0\n",
        "}\n",
        "\n",
        "# Functional dataset creation\n",
        "def create_dataset(df, src2idx, trg2idx, max_len=50):\n",
        "    \"\"\"Create dataset as lists of tensors instead of using a class\"\"\"\n",
        "    src_data = []\n",
        "    trg_data = []\n",
        "\n",
        "    for i in range(len(df)):\n",
        "        src_tokens = [\"<sos>\"] + tokenize(df.iloc[i][\"asm\"]) + [\"<eos>\"]\n",
        "        trg_tokens = [\"<sos>\"] + tokenize(df.iloc[i][\"eng\"]) + [\"<eos>\"]\n",
        "\n",
        "        # Numericalize, map UNK if missing\n",
        "        src_ids = [src2idx.get(w, src2idx[\"<unk>\"]) for w in src_tokens][:max_len]\n",
        "        trg_ids = [trg2idx.get(w, trg2idx[\"<unk>\"]) for w in trg_tokens][:max_len]\n",
        "\n",
        "        src_data.append(torch.tensor(src_ids))\n",
        "        trg_data.append(torch.tensor(trg_ids))\n",
        "\n",
        "    return src_data, trg_data\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "    src_padded = pad_sequence(src_batch, padding_value=asm2idx[\"<pad>\"], batch_first=True)\n",
        "    trg_padded = pad_sequence(trg_batch, padding_value=eng2idx[\"<pad>\"], batch_first=True)\n",
        "    return src_padded, trg_padded\n",
        "\n",
        "# Create functional datasets\n",
        "train_src, train_trg = create_dataset(train_df, asm2idx, eng2idx)\n",
        "val_src, val_trg = create_dataset(val_df, asm2idx, eng2idx)\n",
        "\n",
        "# Create datasets as lists of tuples\n",
        "train_dataset = list(zip(train_src, train_trg))\n",
        "val_dataset = list(zip(val_src, val_trg))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=HYP[\"batch_size\"], shuffle=True, collate_fn=collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=HYP[\"batch_size\"], shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Vu82RzjXVSf",
        "outputId": "7898ddbb-03c2-434b-fab5-b3790bda1fb8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model parameters: 63,887,397\n"
          ]
        }
      ],
      "source": [
        "# Cell: Functional Model Implementation\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Create model components as separate modules\n",
        "def create_encoder(input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.2):\n",
        "    encoder = nn.ModuleDict({\n",
        "        'embedding': nn.Embedding(input_dim, emb_dim, padding_idx=asm2idx['<pad>']),\n",
        "        'dropout': nn.Dropout(dropout),\n",
        "        'lstm': nn.LSTM(emb_dim, hid_dim, num_layers=n_layers,\n",
        "                       bidirectional=True, batch_first=True,\n",
        "                       dropout=dropout if n_layers > 1 else 0)\n",
        "    })\n",
        "    encoder.hid_dim = hid_dim\n",
        "    encoder.n_layers = n_layers\n",
        "    return encoder\n",
        "\n",
        "def create_attention(enc_hid, dec_hid):\n",
        "    attention = nn.ModuleDict({\n",
        "        'attn': nn.Linear(enc_hid*2 + dec_hid, dec_hid),\n",
        "        'v': nn.Linear(dec_hid, 1, bias=False)\n",
        "    })\n",
        "    return attention\n",
        "\n",
        "def create_decoder(output_dim, emb_dim, enc_hid, dec_hid, n_layers=2, dropout=0.2):\n",
        "    decoder = nn.ModuleDict({\n",
        "        'embedding': nn.Embedding(output_dim, emb_dim, padding_idx=eng2idx['<pad>']),\n",
        "        'dropout': nn.Dropout(dropout),\n",
        "        'lstm': nn.LSTM(emb_dim + enc_hid*2, dec_hid, num_layers=n_layers,\n",
        "                       batch_first=True, dropout=dropout if n_layers > 1 else 0),\n",
        "        'layer_norm': nn.LayerNorm(dec_hid + enc_hid*2 + emb_dim),\n",
        "        'fc': nn.Linear(dec_hid + enc_hid*2 + emb_dim, output_dim),\n",
        "        'dropout_out': nn.Dropout(dropout)\n",
        "    })\n",
        "    return decoder\n",
        "\n",
        "# Forward functions\n",
        "def encoder_forward(encoder, src):\n",
        "    # src: [batch, src_len]\n",
        "    emb = encoder['dropout'](encoder['embedding'](src))  # [batch, src_len, emb_dim]\n",
        "    outputs, (hidden, cell) = encoder['lstm'](emb)  # outputs: [batch, src_len, hid*2]\n",
        "\n",
        "    # Combine forward and backward for each layer\n",
        "    hidden = hidden.view(encoder.n_layers, 2, -1, encoder.hid_dim)\n",
        "    hidden = torch.cat([hidden[:, 0], hidden[:, 1]], dim=2)\n",
        "\n",
        "    cell = cell.view(encoder.n_layers, 2, -1, encoder.hid_dim)\n",
        "    cell = torch.cat([cell[:, 0], cell[:, 1]], dim=2)\n",
        "\n",
        "    return outputs, (hidden, cell)\n",
        "\n",
        "def attention_forward(attention, hidden, enc_outputs, src_mask=None):\n",
        "    batch_size, src_len, _ = enc_outputs.size()\n",
        "    h = hidden[-1].unsqueeze(1).repeat(1, src_len, 1)  # Use last layer\n",
        "\n",
        "    energy = torch.tanh(attention['attn'](torch.cat((h, enc_outputs), dim=2)))\n",
        "    attn = attention['v'](energy).squeeze(2)  # [batch, src_len]\n",
        "\n",
        "    # Apply mask if provided\n",
        "    if src_mask is not None:\n",
        "        attn = attn.masked_fill(src_mask == 0, -1e10)\n",
        "\n",
        "    return F.softmax(attn, dim=1)\n",
        "\n",
        "def decoder_forward(decoder, attention, inp, hidden_cell, enc_outputs, src_mask=None):\n",
        "    hidden, cell = hidden_cell\n",
        "    emb = decoder['dropout'](decoder['embedding'](inp)).unsqueeze(1)  # [batch, 1, emb_dim]\n",
        "\n",
        "    # Attention\n",
        "    a = attention_forward(attention, hidden, enc_outputs, src_mask).unsqueeze(1)  # [batch, 1, src_len]\n",
        "    weighted = torch.bmm(a, enc_outputs)  # [batch, 1, enc_hid*2]\n",
        "\n",
        "    # LSTM input\n",
        "    rnn_in = torch.cat((emb, weighted), dim=2)\n",
        "    out, (hidden, cell) = decoder['lstm'](rnn_in, (hidden, cell))\n",
        "\n",
        "    # Prepare output\n",
        "    out = out.squeeze(1)       # [batch, dec_hid]\n",
        "    weighted = weighted.squeeze(1)  # [batch, enc_hid*2]\n",
        "    emb = emb.squeeze(1)       # [batch, emb_dim]\n",
        "\n",
        "    # Layer norm + residual-like connection\n",
        "    concat_out = torch.cat((out, weighted, emb), dim=1)\n",
        "    concat_out = decoder['layer_norm'](concat_out)\n",
        "    pred = decoder['fc'](decoder['dropout_out'](concat_out))\n",
        "\n",
        "    return pred, (hidden, cell)\n",
        "\n",
        "def create_mask(src):\n",
        "    return (src != asm2idx['<pad>']).float()\n",
        "\n",
        "def seq2seq_forward(encoder, decoder, attention, src, trg, teacher_forcing_ratio=0.9):\n",
        "    batch_size, trg_len = trg.size()\n",
        "    vocab_size = len(idx2eng)\n",
        "\n",
        "    # Create source mask\n",
        "    src_mask = create_mask(src)\n",
        "\n",
        "    # Encode\n",
        "    enc_out, hidden_cell = encoder_forward(encoder, src)\n",
        "\n",
        "    # Initialize outputs\n",
        "    outputs = torch.zeros(batch_size, trg_len, vocab_size).to(src.device)\n",
        "    input_tok = trg[:, 0]\n",
        "\n",
        "    for t in range(1, trg_len):\n",
        "        pred, hidden_cell = decoder_forward(decoder, attention, input_tok, hidden_cell, enc_out, src_mask)\n",
        "        outputs[:, t] = pred\n",
        "\n",
        "        # Teacher forcing with probability\n",
        "        use_teacher_forcing = torch.rand(1).item() < teacher_forcing_ratio\n",
        "        if use_teacher_forcing:\n",
        "            input_tok = trg[:, t]\n",
        "        else:\n",
        "            input_tok = pred.argmax(1)\n",
        "\n",
        "    return outputs\n",
        "\n",
        "# Create model components\n",
        "encoder = create_encoder(len(idx2asm), HYP[\"emb_dim\"], HYP[\"hid_dim\"],\n",
        "                        HYP[\"n_layers\"], HYP[\"dropout\"]).to(device)\n",
        "attention = create_attention(HYP[\"hid_dim\"], HYP[\"hid_dim\"] * 2).to(device)\n",
        "decoder = create_decoder(len(idx2eng), HYP[\"emb_dim\"], HYP[\"hid_dim\"],\n",
        "                        HYP[\"hid_dim\"] * 2, HYP[\"n_layers\"], HYP[\"dropout\"]).to(device)\n",
        "\n",
        "# Count parameters\n",
        "def count_parameters(encoder, decoder, attention):\n",
        "    enc_params = sum(p.numel() for p in encoder.parameters() if p.requires_grad)\n",
        "    dec_params = sum(p.numel() for p in decoder.parameters() if p.requires_grad)\n",
        "    att_params = sum(p.numel() for p in attention.parameters() if p.requires_grad)\n",
        "    return enc_params + dec_params + att_params\n",
        "\n",
        "print(f\"Model parameters: {count_parameters(encoder, decoder, attention):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "rAe3v88-XW7A"
      },
      "outputs": [],
      "source": [
        "# Functional Beam Search\n",
        "def beam_search_functional(encoder, decoder, attention, src_sentence, beam_width=HYP[\"beam_width\"], max_len=50, length_penalty=0.6):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    attention.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens = [\"<sos>\"] + src_sentence.split() + [\"<eos>\"]\n",
        "        src_ids = [asm2idx.get(w, asm2idx[\"<unk>\"]) for w in tokens]\n",
        "        src_tensor = torch.tensor(src_ids).unsqueeze(0).to(device)\n",
        "\n",
        "        # Create source mask\n",
        "        src_mask = create_mask(src_tensor)\n",
        "\n",
        "        # Encode\n",
        "        enc_out, hidden_cell = encoder_forward(encoder, src_tensor)\n",
        "\n",
        "        # Initialize beams: (score, sequence, hidden_cell_state)\n",
        "        beams = [(0.0, [eng2idx[\"<sos>\"]], hidden_cell)]\n",
        "        completed = []\n",
        "\n",
        "        for step in range(max_len):\n",
        "            new_beams = []\n",
        "\n",
        "            for score, seq, hc in beams:\n",
        "                if len(seq) > 0 and seq[-1] == eng2idx[\"<eos>\"]:\n",
        "                    completed.append((score, seq))\n",
        "                    continue\n",
        "\n",
        "                last_token = torch.tensor([seq[-1]]).to(device)\n",
        "                pred, hc_new = decoder_forward(decoder, attention, last_token, hc, enc_out, src_mask)\n",
        "\n",
        "                log_probs = F.log_softmax(pred, dim=1).squeeze(0)\n",
        "                topv, topi = log_probs.topk(beam_width)\n",
        "\n",
        "                for i in range(beam_width):\n",
        "                    token_id = topi[i].item()\n",
        "                    token_score = topv[i].item()\n",
        "                    new_seq = seq + [token_id]\n",
        "                    new_score = score + token_score\n",
        "\n",
        "                    new_beams.append((new_score, new_seq, hc_new))\n",
        "\n",
        "            # Keep top beam_width beams\n",
        "            beams = sorted(new_beams, key=lambda x: x[0] / (len(x[1]) ** length_penalty), reverse=True)[:beam_width]\n",
        "\n",
        "            if not beams:\n",
        "                break\n",
        "\n",
        "        # Add remaining beams to completed\n",
        "        for score, seq, _ in beams:\n",
        "            completed.append((score, seq))\n",
        "\n",
        "        if not completed:\n",
        "            return []\n",
        "\n",
        "        # Select best sequence with length normalization\n",
        "        best_seq = max(completed, key=lambda x: x[0] / (len(x[1]) ** length_penalty))[1]\n",
        "\n",
        "        # Convert to words and remove special tokens\n",
        "        result = []\n",
        "        for token_id in best_seq[1:]:  # Skip <sos>\n",
        "            if token_id == eng2idx[\"<eos>\"]:\n",
        "                break\n",
        "            result.append(idx2eng[token_id])\n",
        "\n",
        "        return result\n",
        "\n",
        "\n",
        "def evaluate_functional(encoder, decoder, attention, loader, max_samples=500):\n",
        "    \"\"\"Evaluate with sampling to avoid memory issues\"\"\"\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    attention.eval()\n",
        "    refs, hyps = [], []\n",
        "\n",
        "    sample_count = 0\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            for i in range(src.size(0)):\n",
        "                if sample_count >= max_samples:\n",
        "                    break\n",
        "\n",
        "                # Get source sequence\n",
        "                src_seq = [idx2asm[t.item()] for t in src[i]\n",
        "                          if t.item() not in {asm2idx[\"<pad>\"], asm2idx[\"<sos>\"], asm2idx[\"<eos>\"]}]\n",
        "\n",
        "                # Get reference sequence\n",
        "                trg_seq = [idx2eng[t.item()] for t in trg[i]\n",
        "                          if t.item() not in {eng2idx[\"<pad>\"], eng2idx[\"<sos>\"], eng2idx[\"<eos>\"]}]\n",
        "\n",
        "                # Skip empty sequences\n",
        "                if not src_seq or not trg_seq:\n",
        "                    continue\n",
        "\n",
        "                # Get prediction\n",
        "                pred_seq = beam_search_functional(encoder, decoder, attention, \" \".join(src_seq))\n",
        "\n",
        "                refs.append([trg_seq])\n",
        "                hyps.append(pred_seq if pred_seq else [\"<unk>\"])  # Handle empty predictions\n",
        "\n",
        "                sample_count += 1\n",
        "\n",
        "            if sample_count >= max_samples:\n",
        "                break\n",
        "\n",
        "    if not refs or not hyps:\n",
        "        return 0.0\n",
        "\n",
        "    return corpus_bleu(refs, hyps, smoothing_function=smooth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHqYygerXYuf",
        "outputId": "0159bd1d-51de-43d5-af95-63e54daf8de4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting functional training...\n",
            "Epoch 1, Batch 0/100, Loss: 9.6647\n",
            "Epoch 1, Batch 50/100, Loss: 6.5883\n",
            "Evaluating...\n",
            "Epoch  1 | Train Loss: 6.741 | Val BLEU: 1.57 | LR: 0.001000\n",
            "Epoch 2, Batch 0/100, Loss: 5.8355\n",
            "Epoch 2, Batch 50/100, Loss: 5.7156\n",
            "Evaluating...\n",
            "Epoch  2 | Train Loss: 5.789 | Val BLEU: 3.37 | LR: 0.001000\n",
            "Epoch 3, Batch 0/100, Loss: 5.1365\n",
            "Epoch 3, Batch 50/100, Loss: 5.3201\n",
            "Evaluating...\n",
            "Epoch  3 | Train Loss: 5.178 | Val BLEU: 4.00 | LR: 0.001000\n",
            "Epoch 4, Batch 0/100, Loss: 4.4276\n",
            "Epoch 4, Batch 50/100, Loss: 4.5543\n",
            "Evaluating...\n",
            "Epoch  4 | Train Loss: 4.610 | Val BLEU: 4.06 | LR: 0.001000\n",
            "Epoch 5, Batch 0/100, Loss: 4.0103\n",
            "Epoch 5, Batch 50/100, Loss: 3.9699\n",
            "Evaluating...\n",
            "Epoch  5 | Train Loss: 4.118 | Val BLEU: 4.65 | LR: 0.001000\n",
            "Epoch 6, Batch 0/100, Loss: 3.6955\n",
            "Epoch 6, Batch 50/100, Loss: 3.8205\n",
            "Evaluating...\n",
            "Epoch  6 | Train Loss: 3.740 | Val BLEU: 5.03 | LR: 0.001000\n",
            "Epoch 7, Batch 0/100, Loss: 3.3322\n",
            "Epoch 7, Batch 50/100, Loss: 3.3629\n",
            "Evaluating...\n",
            "Epoch  7 | Train Loss: 3.491 | Val BLEU: 5.37 | LR: 0.001000\n",
            "Epoch 8, Batch 0/100, Loss: 3.0934\n",
            "Epoch 8, Batch 50/100, Loss: 3.1500\n",
            "Evaluating...\n",
            "Epoch  8 | Train Loss: 3.286 | Val BLEU: 5.73 | LR: 0.001000\n",
            "Epoch 9, Batch 0/100, Loss: 2.9742\n",
            "Epoch 9, Batch 50/100, Loss: 3.2069\n",
            "Evaluating...\n",
            "Epoch  9 | Train Loss: 3.147 | Val BLEU: 5.04 | LR: 0.001000\n",
            "Epoch 10, Batch 0/100, Loss: 2.7507\n",
            "Epoch 10, Batch 50/100, Loss: 3.0538\n",
            "Evaluating...\n",
            "Epoch 10 | Train Loss: 2.972 | Val BLEU: 5.99 | LR: 0.001000\n",
            "Epoch 11, Batch 0/100, Loss: 2.5962\n",
            "Epoch 11, Batch 50/100, Loss: 2.7491\n",
            "Evaluating...\n",
            "Epoch 11 | Train Loss: 2.879 | Val BLEU: 6.71 | LR: 0.001000\n",
            "Epoch 12, Batch 0/100, Loss: 2.4539\n",
            "Epoch 12, Batch 50/100, Loss: 2.9851\n",
            "Evaluating...\n",
            "Epoch 12 | Train Loss: 2.743 | Val BLEU: 6.39 | LR: 0.001000\n",
            "Epoch 13, Batch 0/100, Loss: 2.5282\n",
            "Epoch 13, Batch 50/100, Loss: 2.5246\n",
            "Evaluating...\n",
            "Epoch 13 | Train Loss: 2.631 | Val BLEU: 6.78 | LR: 0.001000\n",
            "Epoch 14, Batch 0/100, Loss: 2.4661\n",
            "Epoch 14, Batch 50/100, Loss: 2.8779\n",
            "Evaluating...\n",
            "Epoch 14 | Train Loss: 2.546 | Val BLEU: 6.71 | LR: 0.001000\n",
            "Epoch 15, Batch 0/100, Loss: 2.3295\n",
            "Epoch 15, Batch 50/100, Loss: 2.4569\n",
            "Evaluating...\n",
            "Epoch 15 | Train Loss: 2.431 | Val BLEU: 6.94 | LR: 0.001000\n",
            "Epoch 16, Batch 0/100, Loss: 2.2655\n",
            "Epoch 16, Batch 50/100, Loss: 2.5571\n",
            "Evaluating...\n",
            "Epoch 16 | Train Loss: 2.347 | Val BLEU: 6.86 | LR: 0.001000\n",
            "Epoch 17, Batch 0/100, Loss: 2.3266\n",
            "Epoch 17, Batch 50/100, Loss: 2.3993\n",
            "Evaluating...\n",
            "Epoch 17 | Train Loss: 2.296 | Val BLEU: 7.08 | LR: 0.001000\n",
            "Epoch 18, Batch 0/100, Loss: 2.2935\n",
            "Epoch 18, Batch 50/100, Loss: 2.0918\n",
            "Evaluating...\n",
            "Epoch 18 | Train Loss: 2.223 | Val BLEU: 6.76 | LR: 0.001000\n",
            "Epoch 19, Batch 0/100, Loss: 2.0102\n",
            "Epoch 19, Batch 50/100, Loss: 2.0453\n",
            "Evaluating...\n",
            "Epoch 19 | Train Loss: 2.172 | Val BLEU: 6.52 | LR: 0.001000\n",
            "Epoch 20, Batch 0/100, Loss: 2.0814\n",
            "Epoch 20, Batch 50/100, Loss: 1.9426\n",
            "Evaluating...\n",
            "Epoch 20 | Train Loss: 2.112 | Val BLEU: 7.05 | LR: 0.001000\n",
            "Epoch 21, Batch 0/100, Loss: 1.9445\n",
            "Epoch 21, Batch 50/100, Loss: 1.9289\n",
            "Evaluating...\n",
            "Epoch 21 | Train Loss: 2.076 | Val BLEU: 6.80 | LR: 0.000500\n",
            "Epoch 22, Batch 0/100, Loss: 2.0926\n",
            "Epoch 22, Batch 50/100, Loss: 1.9329\n",
            "Evaluating...\n",
            "Epoch 22 | Train Loss: 1.950 | Val BLEU: 7.20 | LR: 0.000500\n",
            "Epoch 23, Batch 0/100, Loss: 1.6962\n",
            "Epoch 23, Batch 50/100, Loss: 1.7427\n",
            "Evaluating...\n",
            "Epoch 23 | Train Loss: 1.769 | Val BLEU: 7.33 | LR: 0.000500\n",
            "Epoch 24, Batch 0/100, Loss: 1.6696\n",
            "Epoch 24, Batch 50/100, Loss: 1.7819\n",
            "Evaluating...\n",
            "Epoch 24 | Train Loss: 1.728 | Val BLEU: 7.38 | LR: 0.000500\n",
            "Epoch 25, Batch 0/100, Loss: 1.6041\n",
            "Epoch 25, Batch 50/100, Loss: 1.6487\n",
            "Evaluating...\n",
            "Epoch 25 | Train Loss: 1.698 | Val BLEU: 7.37 | LR: 0.000500\n",
            "Epoch 26, Batch 0/100, Loss: 1.9054\n",
            "Epoch 26, Batch 50/100, Loss: 1.7136\n",
            "Evaluating...\n",
            "Epoch 26 | Train Loss: 1.692 | Val BLEU: 7.15 | LR: 0.000500\n",
            "Epoch 27, Batch 0/100, Loss: 1.6228\n",
            "Epoch 27, Batch 50/100, Loss: 1.6392\n",
            "Evaluating...\n",
            "Epoch 27 | Train Loss: 1.653 | Val BLEU: 7.09 | LR: 0.000500\n",
            "Epoch 28, Batch 0/100, Loss: 1.5953\n",
            "Epoch 28, Batch 50/100, Loss: 1.6430\n",
            "Evaluating...\n",
            "Epoch 28 | Train Loss: 1.663 | Val BLEU: 7.14 | LR: 0.000250\n",
            "Epoch 29, Batch 0/100, Loss: 1.5945\n",
            "Epoch 29, Batch 50/100, Loss: 1.6133\n",
            "Evaluating...\n",
            "Epoch 29 | Train Loss: 1.617 | Val BLEU: 7.48 | LR: 0.000250\n",
            "Epoch 30, Batch 0/100, Loss: 1.5279\n",
            "Epoch 30, Batch 50/100, Loss: 1.6204\n",
            "Evaluating...\n",
            "Epoch 30 | Train Loss: 1.577 | Val BLEU: 7.70 | LR: 0.000250\n"
          ]
        }
      ],
      "source": [
        "# Functional Training Loop\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn.utils as utils\n",
        "\n",
        "# Create all parameters list for optimizer\n",
        "all_params = list(encoder.parameters()) + list(decoder.parameters()) + list(attention.parameters())\n",
        "optimizer = optim.Adam(all_params, lr=HYP[\"lr\"], weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, verbose=True)\n",
        "\n",
        "# Replace the label_smoothing_loss function in the training cell\n",
        "def label_smoothing_loss(pred, target, vocab_size, smoothing=0.1, ignore_index=0):\n",
        "    # pred: [batch_size, vocab_size]\n",
        "    # target: [batch_size]\n",
        "\n",
        "    # Create mask for valid tokens (not padding)\n",
        "    mask = (target != ignore_index)\n",
        "\n",
        "    # Filter out padding tokens\n",
        "    pred = pred[mask]\n",
        "    target = target[mask]\n",
        "\n",
        "    if pred.size(0) == 0:  # No valid tokens\n",
        "        return torch.tensor(0.0, device=pred.device, requires_grad=True)\n",
        "\n",
        "    log_prob = F.log_softmax(pred, dim=1)\n",
        "\n",
        "    # Create smoothed target distribution\n",
        "    smooth_target = torch.zeros_like(log_prob)\n",
        "    smooth_target.fill_(smoothing / (vocab_size - 1))\n",
        "\n",
        "    # Set correct class probability\n",
        "    smooth_target.scatter_(1, target.unsqueeze(1), 1.0 - smoothing)\n",
        "\n",
        "    loss = -torch.sum(log_prob * smooth_target, dim=1)\n",
        "    return loss.mean()\n",
        "\n",
        "# Training variables\n",
        "# Removed early stopping variables: best_bleu, patience_counter, early_stop_patience, epochs_since_best\n",
        "\n",
        "print(\"Starting functional training...\")\n",
        "for epoch in range(1, HYP[\"epochs\"] + 1):\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    attention.train()\n",
        "    epoch_loss = 0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for batch_idx, (src, trg) in enumerate(train_loader):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Dynamic teacher forcing ratio\n",
        "        teacher_forcing_ratio = max(0.5, 1.0 - (epoch - 1) * 0.02)\n",
        "\n",
        "        output = seq2seq_forward(encoder, decoder, attention, src, trg, teacher_forcing_ratio)\n",
        "\n",
        "        # Reshape for loss calculation\n",
        "        output = output[:, 1:].reshape(-1, len(idx2eng))\n",
        "        trg_y = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = label_smoothing_loss(output, trg_y, len(idx2eng), smoothing=0.1, ignore_index=eng2idx[\"<pad>\"])\n",
        "        loss.backward()\n",
        "\n",
        "        # Gradient clipping\n",
        "        utils.clip_grad_norm_(all_params, HYP[\"clip_grad\"])\n",
        "\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        # Print progress every 100 batches\n",
        "        if batch_idx % 50 == 0:\n",
        "            print(f'Epoch {epoch}, Batch {batch_idx}/{num_batches}, Loss: {loss.item():.4f}')\n",
        "\n",
        "    # Validation\n",
        "    print(\"Evaluating...\")\n",
        "    val_bleu = evaluate_functional(encoder, decoder, attention, val_loader)\n",
        "    avg_train_loss = epoch_loss / len(train_loader)\n",
        "\n",
        "    # Learning rate scheduling\n",
        "    scheduler.step(val_bleu)\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "\n",
        "    print(f\"Epoch {epoch:2d} | Train Loss: {avg_train_loss:.3f} | Val BLEU: {val_bleu*100:.2f} | LR: {current_lr:.6f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqP1PppjcOrh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
